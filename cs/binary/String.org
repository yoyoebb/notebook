#+SETUPFILE: ~/.emacs.d/org-templates/level-2.org
#+TITLE: String


* 字符、字符集、字符集编码
  名词解释:
  + 字符(Character) :: 各种文字和符号的总称，包括各个国家文字、标点符号、图形符号、数字等。
  + 字符集(Character Set) :: 维护字符及其对应码位关系的集合，字符集种类繁多，不同字符集包含的字符数量、种类也都不一样。
  + 字符集编码 :: 将字符集的码位转化为二进制字节的方式，基于字符集的大小，有单字节编码、双字节编码、可变长字节编码等，同一个字符集也可以有多种编码格式。

** 英文字符集及其编码
*** ASCII
  ASCII（American Standard Code for Information Interchange，美国信息交换标准代码）是基于拉丁字母的一套电脑编码系统，主要用于显示现代英语和其他西欧语言。它是现今最通用的单字节编码系统，并等同于国际标准ISO/IEC 646。

  ASCII字符集最初目的是规定英语环境下常用的字符及通信用的控制字符，因为在当时1bit的存储是很昂贵的，美国佬禀着“够用就好”的原则,采用7bit编码(0-127)，一共128个字符，包括:
  + 0~31 & 127这33个码位被用于 控制字符 或 通信专用字符
  + 32~126这95个码位是可显示字符，包括:
    + 10个阿拉伯数字
    + 52个大小写英语字母
    + 33个特殊字符，如空格、标点符号、运算符号等

  后面随着存储费用的下降，以及编程的简便，人们需要一个更大的单位Byte，并且最终确认了 1Byte=8bit(不用7bit是因为那样存储容量就不再是 2^{n} bit 大小了)，而ASCII编码也就从原来的7bit变为了8bit(最高位补0)，并且规定这个多余的最高位用于奇偶校验。

  ASCII最大的问题就是仅适用于美国佬，无法满足那些拥有不同字母体系、不同符号的其他国家的要求。譬如，同属英语体系的英国佬就会抗议，为啥没有英镑符号呢！另外有效码位才128位，实在是太少了。于是各个国家就根据自己的实际情况，纷纷制定起符合各自国情的字符集，编码方式也按照字符集的大小，采用单字节或双字节编码。这些新出现的字符集通常有个共同的特性，就是它的前128位码位会兼容ASCII码。
*** ISO-8859
  单字节编码明明可以支持256个码位，但ASCII只用了128个，剩下的不用实在是太浪费，于是IBM最先开始扩展剩余的码位来形成新的字符集，并逐渐形成了ISO 8859标准。
  + ISO-8859-1，又被称为Latin-1，是扩展了拉丁语系中的一些关键字符后，形成的一套西方社会通用字符集。
** 中文字符集及其编码
*** GB2312 (1980)
  等到中国人民用上计算机的时候，立刻意识到中文编码的问题，那区区一个字节，最多也就256个码位，怎么放得下老祖宗传下来的几万个汉字，不过这个问题也好解决，一个字节不够用，那就来两个呗。于是相关人士开始基于双字节编码制订GB2312编码规范。
  
  此时ISO组织已经好心地提供了一个ISO 2022标准，它采用7bit编码(字节最高位为0)，并只兼容ASCII的通讯码位、控制码位(#0-#31, #127，共33个)和空格(#32)，因此可用的码位就只剩下128-34=94位了，双字节就是94*94=8836个码位。

  + 区位码 :: GB2312将这94*94定义成了区和位，每个区位对应一个字符。
    + 01-09区 ： 定义了符号、数字、英文字符...等，共682个；
    + 10-15区 ： 未使用，保留；
    + 16-55区 ： 常用汉字(一级汉字)，按拼音排序，共3755个；
    + 56-87区 ： 非常用汉字(二级汉字)，按部首排序，共3008个；
    + 88-94区 ： 未使用，保留；
  + 国标码 :: 将区和位分别加上32(0x20)，就是对应的国标码，范围正好是33~126，如：区位码"01 01"，对应国标码"33 33"
  + 内码 :: 因为ISO 2022标准并不兼容ASCII，而7bit的编码方案也逐渐被时代淘汰，最终微软借鉴ISO 8859体系的做法，采用了一套折衷方案，将国标码两个字节的最高位 置为 1(分别加上128，16进制0x80)，这样终于兼容ASCII码也能满足GB2312的要求。
    + 如果一个字节是0~127，则为对应的ASCII码位
    + 如果一个字节是128~255，则它后面一个字节也是128~255，两个字节组成一个对应的GB2312字符。
*** BIG5(繁体中文)
  主要为香港、台湾地区使用的一套繁体字为主的双字节编码字符集。
*** GBK
  随着越来越多的生僻字的加入，GB2312的码位有点不够用了，大家觉得两个字节都要求在128~255的做法太浪费，于是把低字节最高位的限制拿掉，形成了新的GBK标准，一共收录了21003个汉字，并向下兼容GB2312，向上支持(并不完全兼容) ISO 10646-1(UCS)，还收录了BIG5编码中的所有汉字(不兼容)。
** ANSI
  + SBCS :: 单字节编码字符集
  + DBCS :: 双字节编码字符集，其特点是兼容ASCII
  这些字符集在Window平台被归类为ANSI体系。
** Unicode及其编码
*** Unicode概述
  天下大势，分久必合，合久必分，从ASCII面世以来，各种字符集层出不穷，导致转码、乱码的问题不断发生，终于有人看不下去，决心搞一套大而全，包罗万象的字符集。

  当时存在两个试图独立设计通用字符集的组织：ISO和它的10646标准(UCS)，Unicode.org和Unicode项目。在1991年前后，双方都认识到世界不需要两个不兼容的通用字符集，于是它们开始合并双方的工作成果，为创立一个单一编码表而协同工作。从unicode 2.0开始，Unicode项目就采用了与ISO 10646-1相同的字库和字码。目前的最新版本分别是：Unicode 7.0(2014) 和 ISO 10646:2012

  Unicode一开始规定为两个字节的编码范围(0-65535)，其码位(code point)完全兼容ISO-8859-1，但随着版本的升级及字符集的扩大，开始超出了65535的范围。目前定义了0-16共17个Plane(U+0000 ~ U+10FFFF)，每个Plane都包含65536个码位，最初的(U0 ~ UFFFF)被称为BMP(Basic Multilingual Plane，0号Plane)。
*** Unicode的编码方式
  在Unicode的发展历史中，出现过多种编码方式。
  + UCS-2 :: 最早出现的固定双字节编码方式，当时还是BMP的时代，很自然的把Unicode的每个码位都用双字节来保存，这种做法虽然简单粗暴，但由于计算机发达的西方世界普遍采用ISO 8859-1的单字节编码，改用UCS-2编码会导致存储和传输容量翻倍，也无法兼容老的ASCII编码文件，所以推广缓慢。但很多新的语言如Java，一开始都是直接支持Unicode并采用UCS-2编码。在多位面出现后，UCS-2被UTF-16取代。

  + UTF-8 :: 1992年，Rob Pike和Ken Thompson在18M一个工作小组的提案基础上进行优化，产生了UTF-8编码，它采用可变长度字节对Unicode的code point编码，并且完全兼容ASCII的字节码。
    + 如果字节最高位为0，该字节代表一个code point
    + 如果字节最高位为110,1110...，它和后面n-1个10开头的字节(n为连续1的个数)代表一个code pooint
    + 以10开头的字节都是某个code point的低位字节
    + 字节中的其他位连起来(左补0)，表示对应code point的二进制编码
    | BTYE | UTF-8                         | Unicode Scale(2^{nx}-1) |
    |------+-------------------------------+-------------------------|
    |    1 | 0xxx xxxx                     | 0~127                   |
    |    2 | 110x xxxx 10xx xxxx           | 128~2047                |
    |    3 | 1110 xxxx 10xx xxxx 10xx xxxx | 2048~65535              |
    |  4-6 | ...                           | ...                     |

  + UTF-16 :: 在Unicode超过BMP范围后，UCS-2不再适用，于是规定两个字节作为一个code unit，BMP的每个码位对应一个code unit，扩展Plane的每个码位对应两个code unit，并且专门引入了代理区(Surrogate)的概念。
    + 如果U-x < U-0x10000，按一个code unit正常编码
    + 如果U-x >=U-0x10000，计算U'=U-0x10000，然后将U'写为20bit的二进制形式： ~yyyy yyyy yyxx xxxx xxxx~ ，U-x的UTF-16编码就是： ~1101 10yy yyyy yyyy~ 和 ~1101 11xx xxxx xxxx~ ，可知高位字节以110110开头，取值范围是0xD800~0xDBFF，低位字节以110111开头，取值范围是0xDC00~0xDFFF，Unicode限制0xD800~0xDFFF这段区域的码位不能使用，并将其称为代理区(Surrogate)

  + UTF-32 :: 直接以4个字节作为code unit的编码方式，可以与目前所有的code point一一对应，但由于实在太浪费空间，很少使用。 
*** Byte Order Mark(BOM)
  对于UTF-16和UTF-32，由于CPU体系处理方式的不同，存在两种字节序 Big Endian(BE) 和 Little Endian(LE)，如对于code point 0x6C49，采用BE方式编码后则为6C 49，采用LE方式编码后则为49 6C
  #+BEGIN_QUOTE
  "endian"这个词出自《格列佛游记》，小人国的内战就源于吃鸡蛋时究竟从大头(Bit-Endian)敲开还是从小头(Little-Endian)敲开，为此一共发生了六次叛乱，导致一个皇帝送了命，另一个丢了王位。
  #+END_QUOTE
  
  为了标记编码对应的字节序，Unicode规范中推荐的方法是在文件头加入BOM，BOM是个取巧的做法：在Unicode中，U-0xFEFF对应的字符叫“ZERO WITH NO-BREAK SPACE"，而0xFFFE在Unicode中是不存在的字符，不应出现在实际传输中。因此Unicode规范建议，如果字节流以U-FEFF开头，就表明字节流是BE，如果以U-FFFE开头，就表明字节流是LE。

  因为UTF-8是自解释的，可以无需BOM，Unicode虽然定义了UTF-8的BOM，但并不推荐使用。

  #+CAPTION: BOM规范
  | UTF-8 without BOM | -           |
  | UTF-8 with BOm    | EF BB BF    |
  | UTF-16 BE         | FE FF       |
  | UTF-16 LE         | FF FE       |
  | UTF-32 BE         | 00 00 FE FF |
  | UTF-32 LE         | FF FE 00 00 |
*** Unicode的操作系统支持
 + 内核对Unicode支持(存储和传输还是以UTF-8为主)
   + windows : 从NT开始，内核直接采用Unicode，并使用UTF-16编码
   + linux/unix : 同样支持Unicode，但为了兼容ascii码，只采用UTF-8编码
   + JVM : 支持Unicode，运行时采用UTF-16编码，java文件和class文件使用UTF-8编码
 + UTF-8 with BOM
   + windows : 为了快速区分ANSI文件和UTF文件，微软会在UTF-8文件加入BOM(主要体现在自带的notepad保存UTF-8文件时)
   + linux/unix : 因为*nix的哲学，一切皆文件，而一切文件皆流，一个流可以被任意的切断和独立解析，而不会改变含义，所以它不能有特殊的头和尾。为此linux/unix天然排斥UTF-8加上BOM。（很多批处理文件在从windows复制到linux/unix时，会因为带了BOM头，而导致开头的 ~#!~ 无法被正常解析）
 + UTF-16/32的 BE和LE
   + windows : 缺省是LE(notepad另存为选择的unicode格式其实默认为UTF-16 LE)
   + MAC/JVM : 默认都是BE

* 正则表达式
